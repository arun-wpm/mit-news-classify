{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Classifier Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import math\n",
    "import nltk\n",
    "\n",
    "from keras.layers import Embedding, Lambda, Dense\n",
    "from keras.models import Sequential\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        self.word_to_token = {}\n",
    "        self.token_to_word = {}\n",
    "\n",
    "        self.word_count = {}\n",
    "\n",
    "        self.word_to_token['<unknown>'] = 0\n",
    "        self.token_to_word[0] = '<unknown>'\n",
    "\n",
    "        self.vocabulary = []\n",
    "        self.vocabulary.append('<unknown>')\n",
    "        self.vocab_size = 1\n",
    "\n",
    "        self.min_occur = 30\n",
    "\n",
    "    def get_word_to_token(self):\n",
    "        return self.word_to_token\n",
    "\n",
    "    def get_token_to_word(self):\n",
    "        return self.token_to_word\n",
    "\n",
    "    def build_tokenizer(self, corpus, Cutoff=30):\n",
    "\n",
    "        # only keep word above certain frequency\n",
    "        WordCount = {}\n",
    "        for document in corpus:\n",
    "            document = document.strip().lower()\n",
    "\n",
    "            all_words = nltk.word_tokenize(document)\n",
    "\n",
    "            for word in all_words:\n",
    "                if word not in WordCount:\n",
    "                    WordCount[word] = 1\n",
    "                else:\n",
    "                    WordCount[word] += 1\n",
    "\n",
    "        for Key, Value in WordCount.items():\n",
    "\n",
    "            if Value >= Cutoff:\n",
    "                self.vocabulary.append(Key)\n",
    "                self.word_to_token[Key] = len(self.vocabulary)-1\n",
    "                self.token_to_word[len(self.vocabulary)-1] = Key\n",
    "\n",
    "        print(\"tokenizer with vocab size of \"+str(len(self.vocabulary)))\n",
    "\n",
    "    def fit(self, corpus):\n",
    "        for review in corpus:\n",
    "            review = review.strip().lower()\n",
    "            words = re.findall(r\"[\\w']+|[.,!?;]\", review)\n",
    "            for word in words:\n",
    "                if word not in self.word_count:\n",
    "                    self.word_count[word] = 0\n",
    "                self.word_count[word] += 1\n",
    "\n",
    "        for review in corpus:\n",
    "            review = review.strip().lower()\n",
    "            words = re.findall(r\"[\\w']+|[.,!?;]\", review)\n",
    "            for word in words:\n",
    "                if self.word_count[word] < self.min_occur:\n",
    "                    continue\n",
    "                if word in self.word_to_token:\n",
    "                    continue\n",
    "                self.word_to_token[word] = self.vocab_size\n",
    "                self.token_to_word[self.vocab_size] = word\n",
    "                self.vocab_size += 1\n",
    "\n",
    "    def tokenize(self, corpus):\n",
    "        tokenized = []\n",
    "        for document in corpus:\n",
    "            document = document.strip().lower()\n",
    "            all_words = nltk.word_tokenize(document)\n",
    "            document_tokens = []\n",
    "            for word in all_words:\n",
    "                if word not in self.word_to_token:\n",
    "                    document_tokens.append(0)\n",
    "                else:\n",
    "                    document_tokens.append(self.word_to_token[word])\n",
    "            tokenized.append(document_tokens)\n",
    "        return tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Misc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadcsv(filename):\n",
    "    with open(filename, newline='') as f:\n",
    "        return list(csv.reader(f))\n",
    "\n",
    "\n",
    "def get_topic(number):\n",
    "    if not math.isnan(number):\n",
    "        return int(number)\n",
    "\n",
    "\n",
    "def create_one_hot(item, num_items):\n",
    "    one_hot = np.zeros(num_items)\n",
    "    for thing in item:\n",
    "        one_hot[int(thing)] += 1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def get_mapping(topicfile):\n",
    "    data = loadcsv(topicfile)\n",
    "    id_mapping = {int(t[1]): int(t[0]) for t in data[::2][1:]}\n",
    "    topic_mapping = {int(t[0]): t[2] for t in data[::2][1:]}\n",
    "    return id_mapping, topic_mapping\n",
    "\n",
    "\n",
    "def long_to_short_topic_ids(topics, id_mapping):\n",
    "    new_topics = []\n",
    "    for topic in topics:\n",
    "        if topic in id_mapping:\n",
    "            new_topics.append(id_mapping[topic])\n",
    "    return new_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(xs, ys, vocab_size, max_length, n_batch=500, n_epochs=24):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocab_size, 100, input_length=max_length))\n",
    "    model.add(Lambda(lambda x: K.sum(x, axis=1), input_shape=(max_length, vocab_size)))\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(ys.shape[1], activation='sigmoid'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(xs, ys, batch_size=n_batch, epochs=n_epochs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Input Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "topicfile = \"nyt-theme-tags.csv\"\n",
    "id_mapping, topic_mapping = get_mapping(topicfile)\n",
    "\n",
    "infile = \"../../nyt_corpus/NYTcorpus.p\"\n",
    "articles = pickle.load(open(infile, \"rb\"))\n",
    "\n",
    "texts = [a[2] for a in articles[1:]]\n",
    "long_topics = [list(map(int, a[3:])) for a in articles[1:]]\n",
    "topics = []\n",
    "for topic in long_topics:\n",
    "    topics.append(long_to_short_topic_ids(topic, id_mapping))\n",
    "    \n",
    "num_examples = len(texts)\n",
    "val_index = (7 * num_examples) // 8\n",
    "\n",
    "train_texts = texts[0:val_index]\n",
    "train_labels = topics[0:val_index]\n",
    "\n",
    "test_texts = texts[val_index:]\n",
    "test_labels = topics[val_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just for an example we'll train on 100 examples and test on 50\n",
    "train_texs = train_texts[:100]\n",
    "train_labels = train_labels[:100]\n",
    "\n",
    "test_texts = test_texts[:50]\n",
    "test_labels = test_labels[:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics = 594\n",
    "embed_dim = 500\n",
    "context_size = 5\n",
    "hidden_size = 64\n",
    "learning_rate = 0.001\n",
    "n_epochs = 3\n",
    "n_batch = 500\n",
    "max_length = 25000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit(train_texts)\n",
    "    \n",
    "vocab_size = len(tokenizer.vocabulary)\n",
    "    \n",
    "word_to_token = tokenizer.get_word_to_token()\n",
    "word_to_token.pop(',', None)\n",
    "w = csv.writer(open(\"example/tokenizer.csv\", \"w\"))\n",
    "for key, val in word_to_token.items():\n",
    "    w.writerow([key, val])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_path = \"example/tokenizer.csv\"\n",
    "word_to_token = pd.read_csv(tokenizer_path, header=None, index_col=0, squeeze=True).to_dict()\n",
    "vocab_size = len(word_to_token)\n",
    "tokenized_corpus = tokenize(train_texts, word_to_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare X_train and X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = tokenized_corpus\n",
    "for i, article in enumerate(X_train):\n",
    "    if len(article) < max_length:\n",
    "        padded_article = article + [0]*(max_length - len(article))\n",
    "    else:\n",
    "        padded_article = article[:max_length]\n",
    "    X_train[i] = padded_article\n",
    "    \n",
    "X_test = tokenize(test_texts, word_to_token)\n",
    "for i, article in enumerate(X_test):\n",
    "    if len(article) < max_length:\n",
    "        padded_article = article + [0]*(max_length - len(article))\n",
    "    else:\n",
    "        padded_article = article[:max_length]\n",
    "    X_test[i] = padded_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Y_train and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = []\n",
    "for point in train_labels:\n",
    "    Y_train.append(create_one_hot(point, num_topics))\n",
    "Y_train = np.array(Y_train)\n",
    "\n",
    "Y_test = []\n",
    "for point in test_labels:\n",
    "    Y_test.append(create_one_hot(point, num_topics))\n",
    "Y_test = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train_model(X_train, Y_train, vocab_size, max_length, 250, 6)\n",
    "model.save(\"example/topic_classifier.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Model Output to Topic Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_threshold = 0.25\n",
    "\n",
    "def probabilities_to_onehot(probabilities):\n",
    "\n",
    "    probabilities[np.isnan(probabilities)] = 0\n",
    "\n",
    "    predicted_topics = []\n",
    "    for prob in probabilities:\n",
    "        all_topics = []\n",
    "        topics = np.argwhere(prob > topic_threshold)\n",
    "        for topic in topics:\n",
    "            all_topics.append(topic[0])\n",
    "        predicted_topics.append(all_topics)\n",
    "    return predicted_topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Evaluation Metrics for Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def false_eval(actual_topics, probabilities):\n",
    "    predicted_topics = probabilities_to_onehot(probabilities)\n",
    "    total_actual_topics = 0\n",
    "    false_positives = 0\n",
    "    true_positives = 0\n",
    "\n",
    "    for i in range(len(actual_topics)):\n",
    "        total_actual_topics += len(actual_topics[i])\n",
    "        for topic in predicted_topics[i]:\n",
    "            if str(topic) in actual_topics[i]:\n",
    "                true_positives += 1\n",
    "            else:\n",
    "                false_positives += 1\n",
    "\n",
    "    false_negatives = total_actual_topics - true_positives\n",
    "    return total_actual_topics, false_negatives, false_positives\n",
    "\n",
    "total, f_n, f_p = false_eval(Y_test, model.predict(X_test))\n",
    "print(\"Total topics:\", total)\n",
    "print(\"False Negatives:\", f_n)\n",
    "print(\"False Positives:\", f_p)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
